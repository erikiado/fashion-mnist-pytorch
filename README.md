# Proyecto: Clasificaci√≥n de Fashion MNIST

Este proyecto entrena un modelo de deep learning para clasificar prendas de ropa de Fashion MNIST en 10 clases diferentes. A lo largo del desarrollo, se implementaron varias t√©cnicas para optimizar el desempe√±o del modelo y superar sus l√≠mites iniciales. Este README explica las decisiones tomadas y por qu√© son √∫tiles para resolver este problema, especialmente para principiantes en Python y deep learning.

---

## 1. **Descripci√≥n del Problema**
El conjunto de datos Fashion MNIST contiene 60,000 im√°genes de entrenamiento y 10,000 de prueba de prendas de ropa, clasificadas en 10 clases:

- 0: Camiseta/Top
- 1: Pantal√≥n
- 2: Su√©ter
- 3: Vestido
- 4: Abrigo
- 5: Sandalia
- 6: Camisa
- 7: Zapatilla
- 8: Bolsa
- 9: Bot√≠n

El objetivo es construir un modelo que pueda clasificar correctamente estas im√°genes.

---

## 2. **M√©todos Utilizados para Mejorar el Modelo**

### 2.1 Manejo de Clases Dif√≠ciles o Desbalanceadas
Algunos errores frecuentes del modelo ocurr√≠an entre clases similares, como "Camiseta" (0), "Camisa" (6) y "Pantal√≥n" (1). Para mitigar esto:

- **Funci√≥n de p√©rdida ponderada**: Ajustamos los pesos de las clases para que el modelo preste m√°s atenci√≥n a las clases dif√≠ciles o menos representadas.
  ```python
  from torch.nn import CrossEntropyLoss

  class_weights = torch.tensor([1.0, 1.0, 1.2, 1.0, 1.0, 1.0, 1.5, 1.0, 1.0, 1.0])
  criterion = CrossEntropyLoss(weight=class_weights)
  ```
- **Sobremuestreo**: Aumentamos la cantidad de datos de las clases dif√≠ciles duplicando o generando nuevas muestras artificiales.

### 2.2 Mejoras en los Datos de Entrenamiento

Usamos t√©cnicas de aumento de datos para mejorar la capacidad del modelo de generalizar:

- **Rotaci√≥n, escalado y traslaci√≥n**: Estas transformaciones generan versiones ligeramente diferentes de las im√°genes originales.
- **MixUp y CutMix**: T√©cnicas avanzadas que combinan im√°genes y etiquetas de manera aleatoria, forzando al modelo a aprender patrones m√°s generales.
  ```python
  def mixup_data(x, y, alpha=1.0):
      lam = np.random.beta(alpha, alpha)
      batch_size = x.size()[0]
      index = torch.randperm(batch_size)
      mixed_x = lam * x + (1 - lam) * x[index, :]
      y_a, y_b = y, y[index]
      return mixed_x, y_a, y_b, lam
  ```

### 2.3 Arquitectura del Modelo

1. **Bloques SE**: Estos ayudan al modelo a enfocar su atenci√≥n en las caracter√≠sticas m√°s relevantes de cada canal de la imagen.
2. **M√≥dulos CBAM**: Como alternativa, probamos un m√≥dulo de atenci√≥n m√°s avanzado (Convolutional Block Attention Module).
3. **Modelos m√°s profundos**: Experimentamos con arquitecturas m√°s profundas, como ResNet18, para mejorar la capacidad del modelo.

### 2.4 Ajuste de Hiperpar√°metros
- **Batch size**: Probamos diferentes tama√±os (16, 64) para mejorar la estabilidad del entrenamiento.
- **Optimizadores**: Probamos AdamW y SGD con momentum para encontrar el mejor ajuste.
- **Learning rate scheduler**: Ajustamos la tasa de aprendizaje autom√°ticamente durante el entrenamiento para evitar quedarse atascados en √≥ptimos locales.

### 2.5 Distilaci√≥n de Conocimiento
Entrenamos un modelo m√°s grande ("maestro") y usamos sus predicciones para guiar a nuestro modelo final ("estudiante"). Esto mejora el aprendizaje del modelo peque√±o sin necesidad de entrenarlo desde cero.

### 2.6 Ensambles
Entrenamos varios modelos con ligeras diferencias en sus arquitecturas y combinamos sus predicciones para mejorar la precisi√≥n general.

### 2.7 Transfer Learning
Usamos modelos preentrenados (como ResNet) y los ajustamos finamente a Fashion MNIST para aprovechar patrones ya aprendidos en otras tareas.

---

## 3. **Resultados Obtenidos**
- Precisi√≥n m√°xima alcanzada: **91.39%**.
- Mejor p√©rdida de validaci√≥n: **0.6940**.
- Gr√°ficas de p√©rdida y precisi√≥n muestran que el modelo se acerca a su l√≠mite en esta configuraci√≥n, pero las mejoras aplicadas ayudaron a estabilizar el entrenamiento y evitar el sobreajuste.

<!-- include image from last training kaggle/fashion-mnist/cnnv4/epoch_66_plot-cnn-v4.png  -->
![alt cnn training from 0](cnnv4/epoch_16_plot-cnn-v4.png "CNN Training from 0")
![alt cnn training](cnnv4/epoch_66_plot-cnn-v4.png "CNN Training")

---

## 4. **Conclusi√≥n**
Este proyecto demuestra c√≥mo aplicar t√©cnicas avanzadas de deep learning y optimizaci√≥n para resolver un problema cl√°sico de clasificaci√≥n de im√°genes. Las decisiones tomadas, como el manejo de clases dif√≠ciles, el aumento de datos y el ajuste de hiperpar√°metros, son estrategias importantes para cualquier principiante que busque aprender y mejorar en este campo.

Si tienes dudas o sugerencias, ¬°no dudes en contribuir! üöÄ

